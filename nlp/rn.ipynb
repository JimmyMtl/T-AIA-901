{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des bibliothèques nécessaires\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données d'entraînement\n",
    "# Chaque élément de la liste est une phrase avec des étiquettes indiquant\n",
    "# les points de départ et d'arrivée\n",
    "data = [\n",
    "    {\n",
    "        'sentence': 'Je vais de Paris à Londres en train.',\n",
    "        'start': 'Paris',\n",
    "        'end': 'Londres'\n",
    "    },\n",
    "    {\n",
    "        'sentence': 'Le vol de New York à Los Angeles dure environ cinq heures.',\n",
    "        'start': 'New York',\n",
    "        'end': 'Los Angeles'\n",
    "    },\n",
    "    {\n",
    "        'sentence': 'Le voyage de Rome à Berlin prend environ sept heures en voiture.',\n",
    "        'start': 'Rome',\n",
    "        'end': 'Berlin'\n",
    "    },\n",
    "    {\n",
    "        'sentence': 'Je veux allez à Nancy et je suis a Paris.',\n",
    "        'start': 'Paris',\n",
    "        'end': 'Nancy'\n",
    "    },\n",
    "    {\n",
    "        'sentence': 'Je suis à Metz et je veux allez à Marseille',\n",
    "        'start': 'Metz',\n",
    "        'end': 'Marseille'\n",
    "    },\n",
    "    {\n",
    "        'sentence': 'Je veux allez à Strasbourg depuis Nice',\n",
    "        'start': 'Nice',\n",
    "        'end': 'Strasbourg'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de l'objet Tokenizer pour convertir les phrases en séquences de mots\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([sent['sentence'] for sent in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des phrases en séquences de mots\n",
    "train_sequences = tokenizer.texts_to_sequences([sent['sentence'] for sent in train_data])\n",
    "test_sequences = tokenizer.texts_to_sequences([sent['sentence'] for sent in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mise en forme des séquences pour qu'elles aient toutes la même longueur\n",
    "max_length = max([len(seq) for seq in train_sequences + test_sequences])\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=max_length)\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des modèles de réseau de neurones\n",
    "start_model = tf.keras.Sequential()\n",
    "\n",
    "# Ajout d'une couche d'embedding pour transformer les mots en vecteurs de nombres réels\n",
    "start_model.add(tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 64, input_length=max_length))\n",
    "\n",
    "# Ajout d'une couche de LSTM pour traiter les séquences de mots\n",
    "start_model.add(tf.keras.layers.LSTM(64))\n",
    "\n",
    "# Ajout d'une couche dense pour prédire la sortie\n",
    "start_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilation du modèle\n",
    "start_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guillaume\\Documents\\Epitoc\\MSC3\\T-AIA-901-NCY_1\\Project\\nlp\\rn.ipynb Cellule 8\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guillaume/Documents/Epitoc/MSC3/T-AIA-901-NCY_1/Project/nlp/rn.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Entraînement du modèle\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Guillaume/Documents/Epitoc/MSC3/T-AIA-901-NCY_1/Project/nlp/rn.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m start_model\u001b[39m.\u001b[39;49mfit(train_sequences, [sent[\u001b[39m'\u001b[39;49m\u001b[39mstart\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m sent \u001b[39min\u001b[39;49;00m train_data], epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Guillaume\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Guillaume\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1664\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[1;34m(arrays, validation_split)\u001b[0m\n\u001b[0;32m   1662\u001b[0m unsplitable \u001b[39m=\u001b[39m [\u001b[39mtype\u001b[39m(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m flat_arrays \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _can_split(t)]\n\u001b[0;32m   1663\u001b[0m \u001b[39mif\u001b[39;00m unsplitable:\n\u001b[1;32m-> 1664\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1665\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`validation_split` is only supported for Tensors or NumPy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1666\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39marrays, found following types in the input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(unsplitable)\n\u001b[0;32m   1667\u001b[0m     )\n\u001b[0;32m   1669\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(t \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m flat_arrays):\n\u001b[0;32m   1670\u001b[0m     \u001b[39mreturn\u001b[39;00m arrays, arrays\n",
      "\u001b[1;31mValueError\u001b[0m: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Entraînement du modèle\n",
    "start_model.fit(train_sequences, [sent['start'] for sent in train_data], epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation du modèle sur les données de test\n",
    "start_predictions = start_model.predict(test_sequences)\n",
    "start_accuracy = nltk.accuracy(start_predictions, [sent['start'] for sent in test_data])\n",
    "print('Exactitude pour les points de départ : %.2f' % start_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répéter les étapes ci-dessus pour entraîner et évaluer le modèle pour les points d'arrivée\n",
    "end_model = tf.keras.Sequential()\n",
    "end_model.add(tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 64, input_length=max_length))\n",
    "end_model.add(tf.keras.layers.LSTM(64))\n",
    "end_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "end_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "end_model.fit(train_sequences, [sent['end'] for sent in train_data], epochs=5, validation_split=0.2)\n",
    "end_predictions = end_model.predict(test_sequences)\n",
    "end_accuracy = nltk.accuracy(end_predictions, [sent['end'] for sent in test_data])\n",
    "print('Exactitude pour les points d\\'arrivée : %.2f' % end_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eae5f59d8fba25bcfef2dd471563a815bcd0a3a96804762ddf906651e24b5951"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
